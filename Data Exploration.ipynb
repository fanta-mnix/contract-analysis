{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import functools\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import display, display_markdown\n",
    "from IPython.html import widgets\n",
    "from ipywidgets import *\n",
    "\n",
    "from util import *\n",
    "from text import *\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking a document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalize_text = functools.partial(chain, [normalize_symbols, remove_diacritics, remove_headers, remove_emphasis])\n",
    "\n",
    "tos_500px = normalize_text(read_all('data/amazon_privacy_notice.md'))\n",
    "sents = pd.Series(nltk.sent_tokenize(tos_500px))\n",
    "\n",
    "display_markdown('#### Examples of sentences:', raw=True)\n",
    "for sent in sents[80:100]:\n",
    "    display_markdown(sent, raw=True)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sents = sents.apply(word_tokenize)\n",
    "sent_lengths = tokenized_sents.apply(len)\n",
    "display(sent_lengths.hist(bins=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(sents[10])\n",
    "print('---')\n",
    "print('|'.join(tokenized_sents[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of small and large sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_sents = sents[sent_lengths < 10]\n",
    "display_markdown('#### Small sentences:', raw=True)\n",
    "for sent in small_sents[:2]:\n",
    "    display_markdown(sent, raw=True)\n",
    "    print('---')\n",
    "\n",
    "display_markdown('#### Large sentences:', raw=True)\n",
    "large_sents = sents[sent_lengths > 100]\n",
    "for sent in large_sents[:2]:\n",
    "    display_markdown(sent, raw=True)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming tokens\n",
    "Removing numbers and stopwords and converting remaining tokens to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compare(seq1, seq2, title1, title2):\n",
    "    dict_data = {title1: seq1, title2: seq2}\n",
    "    dict_data = dict([(k, pd.Series(v)) for k, v in dict_data.iteritems()])\n",
    "    return pd.DataFrame(dict_data)\n",
    "\n",
    "\n",
    "process_tokens = functools.partial(chain, [remove_numbers, lowercase, remove_stopwords, list])\n",
    "processed_sents = tokenized_sents.apply(process_tokens)\n",
    "sent_lengths = processed_sents.apply(len)\n",
    "\n",
    "display(compare(tokenized_sents[10], processed_sents[10], 'Before', 'After'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSA (TruncatedSVD) for Topic Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return process_tokens(word_tokenize(text))\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False, tokenizer=tokenizer, ngram_range=(1, 1), min_df=1, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "vectorized_sents = vectorizer.fit_transform(sents[sent_lengths > 5])\n",
    "print(\"Number of sentences after cleanup: {}\".format(vectorized_sents.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_explained_variance(svd):\n",
    "    cum_explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "    plt.plot(range(1, svd.n_components + 1), cum_explained_variance)\n",
    "    plt.xlabel(\"# of Components\")\n",
    "    plt.ylabel(\"Total Explained Variance\")\n",
    "    plt.grid()\n",
    "\n",
    "lsa = TruncatedSVD(100).fit(vectorized_sents)  # 120 to 200\n",
    "plot_explained_variance(lsa)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(60)  # 120 to 200\n",
    "lsa_sents = lsa.fit_transform(vectorized_sents)\n",
    "\n",
    "def inspect_component(component, order):\n",
    "    weights = lsa.components_[component]\n",
    "    should_reverse = True if order == 'Desc' else False\n",
    "    sorted_weights = sorted(zip(vectorizer.get_feature_names(), weights), key=lambda x: x[1], reverse=should_reverse)\n",
    "    return pd.DataFrame(sorted_weights[:15], columns=['Term', 'Weight'])\n",
    "\n",
    "dropdown_component = widgets.Dropdown(options=range(0, lsa.n_components), value=0, description='Comp')\n",
    "select_order = widgets.Select(options=['Asc', 'Desc'], value='Desc', description='Order')\n",
    "widgets.interactive(inspect_component, component=dropdown_component, order=select_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "d39c121793164bf58504eae0305262d9": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
